{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xaff8MbZ1P8y"
      },
      "outputs": [],
      "source": [
        "!pip install yt-dlp\n",
        "!pip install faster-whisper\n",
        "!pip install fpdf\n",
        "!pip install requests\n",
        "!pip install pdfplumber python-docx\n",
        "!pip install pyngrok aiohttp nest_asyncio flask flask-cors\n",
        "!apt-get install fonts-dejavu-core ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import requests\n",
        "import textwrap\n",
        "import glob\n",
        "import time\n",
        "import asyncio\n",
        "import aiohttp\n",
        "from urllib.parse import urlparse, parse_qs, urlencode, urlunparse\n",
        "import concurrent.futures\n",
        "\n",
        "import yt_dlp  # using yt-dlp for audio download\n",
        "from faster_whisper import WhisperModel\n",
        "from fpdf import FPDF\n",
        "\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "from pyngrok import ngrok\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# ===============================\n",
        "# Configuration\n",
        "# ===============================\n",
        "API_KEY = \"open-routeriapitoken\"  # Replace with your actual key\n",
        "CHUNK_SIZE = 10000\n",
        "# ===============================\n",
        "# Global Prompt for Summarization\n",
        "# ===============================\n",
        "SYSTEM_PROMPT = (\n",
        "    \"You are an AI assistant that summarizes and structures information from multiple sources \"\n",
        "    \"(YouTube videos, textbooks, and documents) into an easy-to-read format.\\n\\n\"\n",
        "    \"Given an input (a transcript, textbook chapter, or document), generate a structured summary that \"\n",
        "    \"scales with the length of the input. The format should be:\\n\\n\"\n",
        "    \"1. **Title & Metadata**\\n\"\n",
        "    \"   - Source Type: (Video, Book, or Document)\\n\"\n",
        "    \"   - Title: [Extracted from input]\\n\"\n",
        "    \"   - Author/Speaker: [If available]\\n\"\n",
        "    \"   - Date: [If available]\\n\"\n",
        "    \"   - Duration/Page Count: [Estimated from input]\\n\\n\"\n",
        "    \"2. **Quick Summary** (Short and concise for small inputs, more detailed for long inputs).\\n\\n\"\n",
        "    \"3. **Structured Breakdown:**\\n\"\n",
        "    \"   - For videos: Timeline-based summary with timestamps.\\n\"\n",
        "    \"   - For books/documents: Section-based summary with chapter titles and key points.\\n\\n\"\n",
        "    \"4. **Key Concepts & Definitions** (Table format).\\n\\n\"\n",
        "    \"5. **Self-Check Questions** (3-5 questions, scaling with content depth).\\n\\n\"\n",
        "    \"6. **Actionable Insights** (What to do & common mistakes to avoid).\\n\\n\"\n",
        "    \"7. **Further Learning** (Related videos, books, and resources).\\n\\n\"\n",
        "    \"Adjust the depth of explanation, number of bullet points, and breakdown sections proportionally \"\n",
        "    \"to the length of the input. Ensure clarity, conciseness, and logical organization.\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "5TTJpRDZ1-w-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Helper Functions for Different Input Types\n",
        "# ===============================\n",
        "def download_audio(youtube_url):\n",
        "    \"\"\"\n",
        "    Downloads the best audio from a YouTube URL using yt-dlp.\n",
        "    Cleans the URL and uses a cookies file (cookies.txt) if available.\n",
        "    Returns the downloaded file path.\n",
        "    \"\"\"\n",
        "    parsed = urlparse(youtube_url)\n",
        "    query_params = parse_qs(parsed.query)\n",
        "    if 'v' in query_params:\n",
        "        clean_query = {'v': query_params['v'][0]}\n",
        "        parsed = parsed._replace(query=urlencode(clean_query))\n",
        "        youtube_url = urlunparse(parsed)\n",
        "        print(\"Cleaned URL:\", youtube_url)\n",
        "\n",
        "    ydl_opts = {\n",
        "        'format': 'bestaudio/best',\n",
        "        'outtmpl': 'audio.%(ext)s',\n",
        "        'quiet': False\n",
        "    }\n",
        "    # Use cookies if a cookies.txt file is present\n",
        "    cookies_file = \"cookies.txt\"\n",
        "    if os.path.exists(cookies_file):\n",
        "        ydl_opts[\"cookiefile\"] = cookies_file\n",
        "        print(\"Using cookies file:\", cookies_file)\n",
        "\n",
        "    print(\"Downloading audio from:\", youtube_url)\n",
        "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "        try:\n",
        "            ydl.download([youtube_url])\n",
        "        except Exception as e:\n",
        "            print(\"yt-dlp encountered an error:\", e)\n",
        "\n",
        "    files = glob.glob(\"audio.*\")\n",
        "    if files:\n",
        "        return files[0]\n",
        "    else:\n",
        "        raise FileNotFoundError(\"No audio file found after download.\")\n",
        "\n",
        "def transcribe_audio(audio_path):\n",
        "    \"\"\"\n",
        "    Transcribes audio using faster-whisper on GPU.\n",
        "    \"\"\"\n",
        "    model = WhisperModel(\"base\", device=\"cuda\")\n",
        "    segments, _ = model.transcribe(audio_path)\n",
        "    transcript = \"\\n\".join([segment.text for segment in segments])\n",
        "    return transcript\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    import pdfplumber\n",
        "    all_text = \"\"\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            text = page.extract_text()\n",
        "            if text:\n",
        "                all_text += text + \"\\n\"\n",
        "    return all_text\n",
        "\n",
        "def extract_text_from_docx(docx_path):\n",
        "    import docx\n",
        "    doc = docx.Document(docx_path)\n",
        "    full_text = [para.text for para in doc.paragraphs]\n",
        "    return \"\\n\".join(full_text)\n",
        "\n",
        "def chunk_text(text, max_chars=CHUNK_SIZE):\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(text):\n",
        "        end = start + max_chars\n",
        "        chunks.append(text[start:end])\n",
        "        start = end\n",
        "    return chunks\n"
      ],
      "metadata": {
        "id": "UxgnRNaP2H19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Asynchronous Summarization Functions\n",
        "# ===============================\n",
        "async def async_summarize_text(session, text):\n",
        "    payload = {\n",
        "        \"model\": \"deepseek/deepseek-r1-distill-llama-70b:free\",\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\", \"content\": text}\n",
        "        ],\n",
        "    }\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"HTTP-Referer\": \"https://www.kaggle.com\",\n",
        "        \"X-Title\": \"Multi-Source Summarizer\"\n",
        "    }\n",
        "    async with session.post(\"https://openrouter.ai/api/v1/chat/completions\", json=payload, headers=headers) as resp:\n",
        "        resp_json = await resp.json()\n",
        "        print(\"Async LLM API Response:\", resp_json)\n",
        "        return resp_json.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
        "\n",
        "async def parallel_summarize(chunks):\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        tasks = [asyncio.create_task(async_summarize_text(session, chunk)) for chunk in chunks]\n",
        "        results = await asyncio.gather(*tasks)\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "ejn6KK-e2iBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Synchronous Summarization Function\n",
        "# ===============================\n",
        "def summarize_text_sync(text):\n",
        "    payload = {\n",
        "        \"model\": \"deepseek/deepseek-r1-distill-llama-70b:free\",\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\", \"content\": text}\n",
        "        ],\n",
        "    }\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"HTTP-Referer\": \"https://www.kaggle.com\",\n",
        "        \"X-Title\": \"Multi-Source Summarizer\"\n",
        "    }\n",
        "    response = requests.post(\"https://openrouter.ai/api/v1/chat/completions\", headers=headers, data=json.dumps(payload))\n",
        "    print(\"LLM API Response:\", response.json())\n",
        "    return response.json().get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
        "\n",
        "def save_as_pdf(text, filename=\"YouTube_Notes.pdf\"):\n",
        "    pdf = FPDF()\n",
        "    pdf.set_auto_page_break(auto=True, margin=15)\n",
        "    pdf.add_page()\n",
        "    try:\n",
        "        pdf.add_font('DejaVu', '', '/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf', uni=True)\n",
        "        pdf.set_font('DejaVu', '', 12)\n",
        "    except RuntimeError:\n",
        "        pdf.set_font(\"Arial\", size=12)\n",
        "    for line in text.split(\"\\n\"):\n",
        "        pdf.multi_cell(0, 10, line)\n",
        "    pdf.output(filename, \"F\")\n",
        "    print(f\"✅ PDF saved as {filename}\")\n",
        "    return filename\n",
        "\n",
        "def save_as_markdown(text, filename=\"YouTube_Notes.md\"):\n",
        "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
        "        file.write(text)\n",
        "    print(f\"✅ Markdown file saved as {filename}\")\n",
        "    return filename\n"
      ],
      "metadata": {
        "id": "QbWD3snU2klJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Main Modular Pipeline Function\n",
        "# ===============================\n",
        "def modular_pipeline(input_source, input_type):\n",
        "    timings = {}\n",
        "    start_total = time.time()\n",
        "\n",
        "    if input_type.lower() == \"youtube\":\n",
        "        start_download = time.time()\n",
        "        audio_path = download_audio(input_source)\n",
        "        end_download = time.time()\n",
        "        timings[\"download_audio\"] = end_download - start_download\n",
        "\n",
        "        start_transcribe = time.time()\n",
        "        extracted_text = transcribe_audio(audio_path)\n",
        "        end_transcribe = time.time()\n",
        "        timings[\"transcription\"] = end_transcribe - start_transcribe\n",
        "\n",
        "    elif input_type.lower() == \"pdf\":\n",
        "        start_pdf = time.time()\n",
        "        extracted_text = extract_text_from_pdf(input_source)\n",
        "        end_pdf = time.time()\n",
        "        timings[\"pdf_extraction\"] = end_pdf - start_pdf\n",
        "\n",
        "    elif input_type.lower() == \"docx\":\n",
        "        start_docx = time.time()\n",
        "        extracted_text = extract_text_from_docx(input_source)\n",
        "        end_docx = time.time()\n",
        "        timings[\"docx_extraction\"] = end_docx - start_docx\n",
        "\n",
        "    elif input_type.lower() == \"text\":\n",
        "        if os.path.exists(input_source):\n",
        "            with open(input_source, \"r\", encoding=\"utf-8\") as f:\n",
        "                extracted_text = f.read()\n",
        "        else:\n",
        "            extracted_text = input_source\n",
        "        timings[\"text_extraction\"] = 0\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported input type. Choose from 'youtube', 'pdf', 'docx', 'text'.\")\n",
        "\n",
        "    # Use asynchronous summarization for long inputs\n",
        "    if len(extracted_text) > CHUNK_SIZE:\n",
        "        chunks = chunk_text(extracted_text, max_chars=CHUNK_SIZE)\n",
        "        start_chunks = time.time()\n",
        "        chunk_summaries = asyncio.run(parallel_summarize(chunks))\n",
        "        end_chunks = time.time()\n",
        "        timings[\"chunked_summarization\"] = end_chunks - start_chunks\n",
        "\n",
        "        combined_summary = \"\\n\\n\".join(chunk_summaries)\n",
        "        print(\"Combined Chunk Summary:\\n\", combined_summary)  # Debug print\n",
        "\n",
        "        start_final = time.time()\n",
        "        final_summary = summarize_text_sync(combined_summary)\n",
        "        end_final = time.time()\n",
        "        timings[\"final_summarization\"] = end_final - start_final\n",
        "\n",
        "        # Fallback: if final_summary is empty, use combined_summary directly\n",
        "        if not final_summary.strip():\n",
        "            print(\"Final summary empty, using combined summary as fallback.\")\n",
        "            final_summary = combined_summary\n",
        "            timings[\"final_summarization\"] = 0\n",
        "    else:\n",
        "        start_sum = time.time()\n",
        "        final_summary = summarize_text_sync(extracted_text)\n",
        "        end_sum = time.time()\n",
        "        timings[\"summarization\"] = end_sum - start_sum\n",
        "        chunk_summaries = [final_summary]\n",
        "\n",
        "    total_time = time.time() - start_total\n",
        "    timings[\"total_time\"] = total_time\n",
        "\n",
        "    pdf_file = save_as_pdf(final_summary)\n",
        "    md_file = save_as_markdown(final_summary)\n",
        "\n",
        "    return {\n",
        "        \"final_summary\": final_summary,\n",
        "        \"chunk_summaries\": chunk_summaries,\n",
        "        \"extracted_text_length\": len(extracted_text),\n",
        "        \"timings\": timings,\n",
        "        \"pdf_file\": pdf_file,\n",
        "        \"markdown_file\": md_file\n",
        "    }\n"
      ],
      "metadata": {
        "id": "L3NZaWAq2ntq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Flask App Integration\n",
        "# ===============================\n",
        "app = Flask(__name__)\n",
        "CORS(app)  # Enable CORS for cross-origin requests\n",
        "\n",
        "@app.route(\"/summarize\", methods=[\"POST\"])\n",
        "def summarize_endpoint():\n",
        "    # Check if the request is multipart/form-data (i.e. file upload)\n",
        "    if request.content_type.startswith(\"multipart/form-data\"):\n",
        "        file = request.files.get(\"file\")\n",
        "        input_type = request.form.get(\"input_type\") or \"pdf\"  # Default to pdf if not provided\n",
        "        if not file:\n",
        "            return jsonify({\"error\": \"No file provided\"}), 400\n",
        "        # Save the uploaded file\n",
        "        filename = file.filename\n",
        "        file.save(filename)\n",
        "        input_source = filename\n",
        "    else:\n",
        "        data = request.get_json()\n",
        "        print(\"Received JSON:\", data)\n",
        "        if not data:\n",
        "            return jsonify({\"error\": \"No JSON data provided\"}), 400\n",
        "        input_type = data.get(\"input_type\")\n",
        "        input_source = data.get(\"input_source\")\n",
        "        if not input_type or not input_source:\n",
        "            return jsonify({\"error\": \"Missing required parameters: input_type and input_source\"}), 400\n",
        "\n",
        "    try:\n",
        "        result = modular_pipeline(input_source, input_type)\n",
        "        return jsonify(result)\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n"
      ],
      "metadata": {
        "id": "XYTT8MCi2sGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Run Flask with ngrok\n",
        "# ===============================\n",
        "public_url = ngrok.connect(5000)\n",
        "print(\"Public URL:\", public_url)\n",
        "app.run(port=5000)"
      ],
      "metadata": {
        "id": "U634_oNt2vAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken ngrok_token"
      ],
      "metadata": {
        "id": "ChSJISbo20ue"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}